<!DOCTYPE html SYSTEM "about:legacy-compat">
<html lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops"><head><title>The Mathematical Foundations of Randomness</title><meta content="text/html; charset=utf-8" http-equiv="content-type"/><link href="../css/springer_epub.css" rel="styleSheet" type="text/css"/></head><body epub:type="chapter"><div class="ChapterContextInformation"><div class="ContextInformation" id="Chap3"><div class="ChapterCopyright">© The Author(s) 2016</div><span class="ContextInformationAuthorEditorNames"><span class="Editor"><span class="EditorName">Klaas Landsman</span> and </span><span class="Editor"><span class="EditorName">Ellen van Wolde</span></span><span class="CollaboratorDesignation"> (eds.)</span></span><span class="ContextInformationBookTitles"><span class="BookTitle" epub:type="title" lang="en">The Challenge of Chance</span></span><span class="ContextInformationSeries"><span class="SeriesTitle" lang="en">The Frontiers Collection</span></span><span class="ChapterDOI"><a href="https://doi.org/10.1007/978-3-319-26300-7_3">https://doi.org/10.1007/978-3-319-26300-7_3</a></span></div></div><!--Begin Abstract--><div class="MainTitleSection"><h1 class="ChapterTitle" lang="en">The Mathematical Foundations of Randomness</h1></div><div class="AuthorGroup"><div class="AuthorNames"><span class="Author"><span class="AuthorName">Sebastiaan A. Terwijn</span><sup><a href="#Aff10">1</a> <a href="#ContactOfAuthor1"><span class="ContactIcon"> </span></a></sup></span></div><div class="Affiliations"><div class="Affiliation" id="Aff10"><span class="AffiliationNumber">(1)</span><div class="AffiliationText">Department of Mathematics, Radboud University, P.O. Box 9010, 6500 Nijmegen, GL, The Netherlands</div></div><div class="ClearBoth"> </div></div><div class="Contacts"><div class="Contact" id="ContactOfAuthor1"><div class="ContactIcon"> </div><div class="ContactAuthorLine"><span class="AuthorName">Sebastiaan A. Terwijn</span></div><div class="ContactAdditionalLine"><span class="ContactType">Email: </span><a href="mailto:terwijn@math.ru.nl">terwijn@math.ru.nl</a></div></div></div></div><section class="Abstract" id="Abs1" lang="en"><h2 class="Heading">Abstract</h2><p class="Para" id="Par1">We give a nontechnical account of the mathematical theory of randomness. The theory of randomness is founded on computability theory, and it is nowadays often referred to as algorithmic randomness. It comes in two varieties: A theory of finite objects, that emerged in the 1960s through the work of Solomonoff, Kolmogorov, Chaitin and others, and a theory of infinite objects (starting with von Mises in the early 20th century, culminating in the notions introduced by Martin-Löf and Schnorr in the 1960s and 1970s) and there are many deep and beautiful connections between the two. Research in algorithmic randomness connects computability and complexity theory with mathematical logic, proof theory, probability and measure theory, analysis, computer science, and philosophy. It also has surprising applications in a variety of fields, including biology, physics, and linguistics. Founded on the theory of computation, the study of randomness has itself profoundly influenced computability theory in recent years.</p></section><!--End Abstract--><div class="Fulltext"><section class="Section1 RenderAsSection1" id="Sec1"><h2 class="Heading"><span class="HeadingNumber">1 </span>Introduction</h2><p class="Para" id="Par2">In this chapter we aim to give a nontechnical account of the mathematical theory of randomness. This theory can be seen as an extension of classical probability theory that allows us to talk about individual random objects. Besides answering the philosophical question what it <em class="EmphasisTypeItalic ">means</em> to be random, the theory of randomness has applications ranging from biology, computer science, physics, and linguistics, to mathematics itself.</p><p class="Para" id="Par3">The theory comes in two flavors: A theory of randomness for finite objects (for which the textbook by Li and Vitányi <span class="CitationRef"><a epub:type="noteref" href="#CR16">2008</a></span> is the standard reference) and a theory for infinite ones. The latter theory, as well as the relation between the two theories of randomness, is surveyed in the paper (Downey et al. <span class="CitationRef"><a epub:type="noteref" href="#CR9">2006</a></span>), and developed more in full in the recent textbooks by Downey and Hirschfeldt (<span class="CitationRef"><a epub:type="noteref" href="#CR8">2010</a></span>) and Nies (<span class="CitationRef"><a epub:type="noteref" href="#CR19">2009</a></span>). Built on the theory of computation, the theory of randomness has itself deeply influenced computability theory in recent years.</p><p class="Para" id="Par4">We warn the reader who is afraid of mathematics that there will be formulas and mathematical notation, but we promise that they will be <em class="EmphasisTypeItalic ">explained</em> at a nontechnical level. Some more background information about the concepts involved is given in footnotes and in two appendices. It is fair to say, however, that to come to a better understanding of the subject, there is of course no way around the formulas, and we quote Euclid, who supposedly told King Ptolemy I, when the latter asked about an easier way of learning geometry than Euclid’s Elements, that “there is no royal road to geometry”.<sup><a epub:type="noteref" href="#Fn1" id="Fn1_source">1</a></sup>
</p></section><section class="Section1 RenderAsSection1" id="Sec2"><h2 class="Heading"><span class="HeadingNumber">2 </span>What Is Randomness?</h2><p class="Para" id="Par6">Classical probability theory talks about random objects, for example by saying that if you randomly select four cards from a standard deck, the probability of getting four aces is very small. However, every configuration of four cards has the <em class="EmphasisTypeItalic ">same</em> small probability of appearing, so there is no qualitative difference between individual configurations in this setting. Similarly, if we flip a fair coin one hundred times, and we get a sequence of one hundred tails in succession, we may feel that this outcome is very special, but how do we justify our excitement over this outcome? Is the probability for this outcome not exactly the same as that of any other sequence of one hundred heads and tails?</p><div class="Para" id="Par7">Probability theory has been, and continues to be, a highly successful theory, with applications in almost every branch of mathematics. It was put on a sound mathematical foundation in (<span class="CitationRef"><a epub:type="noteref" href="#CR14">1933</a></span>) by Kolmogorov, and in its modern formulation it is part of the branch of mathematics called <em class="EmphasisTypeItalic ">measure theory</em>. (See Appendix A.) In this form it allows us to also talk not only about randomness in discrete domains (such as cards and coin flips), but also in continuous domains such as numbers on the real line. However, it is important to realize that even in this general setting, probability theory is a theory about <em class="EmphasisTypeItalic ">sets</em> of objects, not of individual objects. In particular, it does not answer the question what an <em class="EmphasisTypeItalic ">individual</em> random object is, or how we could call a sequence of fifty zero’s less random than any other sequence of the same length. Consider the following two sequences of coin flips, where 0 stands for heads and 1 for tails:<div class="Equation" id="Equ3"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\begin{aligned}&amp;amp;0000000000 0000000000 0000000000 0000000000 0000000000 \\&amp;amp;0000111001 1111011110 0111100100 1010111100 1111010111 \end{aligned}$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_Equ3.png" style="height:3.88em"/></div></div></div></div>The first sequence consists of fifty 0’s, and the second was obtained by flipping a coin fifty times.<sup><a epub:type="noteref" href="#Fn2" id="Fn2_source">2</a></sup> Is there any way in which we can make our feeling that the first sequence is special, and that the second is less so, mathematically precise?</div></section><section class="Section1 RenderAsSection1" id="Sec3"><h2 class="Heading"><span class="HeadingNumber">3 </span>Can Randomness Be Defined?</h2><p class="Para" id="Par9">A common misconception about the notion of randomness is that it cannot be formally defined, by applying a tautological reasoning of the form: As soon as something can be precisely defined, it ceases to be random. The following quotation by the Dutch topologist Freudenthal (<span class="CitationRef"><a epub:type="noteref" href="#CR11">1969</a></span>) (taken from van Lambalgen <span class="CitationRef"><a epub:type="noteref" href="#CR25">1987</a></span>) may serve to illustrate this point:</p><div class="Para" id="Par10">
              <blockquote class="BlockQuote"><p class="Para" id="Par11"> It may be taken for granted that any attempt at defining disorder in a formal way will lead to a contradiction. This does not mean that the notion of disorder is contradictory. It is so, however, as soon as I try to formalize it.</p></blockquote>
            </div><p class="Para" id="Par12">A recent discussion of randomness and definability, and what can happen if we equate “random” with “not definable”, is in Doyle (<span class="CitationRef"><a epub:type="noteref" href="#CR10">2011</a></span>).<sup><a epub:type="noteref" href="#Fn3" id="Fn3_source">3</a></sup> The problem is not that the notion of definability is inherently vague (because it is not), but that no <em class="EmphasisTypeItalic ">absolute</em> notion of randomness can exist, and that in order to properly define the notion, one has to specify <em class="EmphasisTypeItalic ">with respect to what</em> the supposed random objects should be random. This is precisely what happens in the modern theory of randomness: A random object is defined as an object that is random with respect to a given type of definition, or class of sets. As the class may vary, this yields a <em class="EmphasisTypeItalic ">scale</em> of notions of randomness, which may be adapted to the specific context in which the notion is to be applied.</p><div class="Para" id="Par14">The first person to attempt to give a mathematical definition of randomness was von Mises (<span class="CitationRef"><a epub:type="noteref" href="#CR28">1919</a></span>), and his proposed definition met with a great deal of opposition of the kind indicated above. Von Mises formalized the intuition that a random sequence should be <em class="EmphasisTypeItalic ">unpredictable</em>. Without giving technical details, his definition can be described as follows. Suppose that <em class="EmphasisTypeItalic ">X</em> is an infinite binary sequence, that is, a sequence<div class="Equation" id="Equ4"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$ X(0), X(1), X(2), X(3), {\ldots } $$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_Equ4.png" style="height:1.86em"/></div></div></div></div>where for each positive integer <em class="EmphasisTypeItalic ">n</em>, <em class="EmphasisTypeItalic ">X</em>(<em class="EmphasisTypeItalic ">n</em>) is either 0 or 1. Suppose further that the values of <em class="EmphasisTypeItalic ">X</em> are unknown to us. We now play a game: At every stage of the game we point to a new location <em class="EmphasisTypeItalic ">n</em> in the sequence, and then the value of <em class="EmphasisTypeItalic ">X</em>(<em class="EmphasisTypeItalic ">n</em>) is revealed to us. Now, according to von Mises, for <em class="EmphasisTypeItalic ">X</em> to be called random, we should not be able to predict in this way the values of <em class="EmphasisTypeItalic ">X</em> with probability better than <span class="InlineEquation" id="IEq1"><img alt="$$\frac{1}{2}$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq1.png" style="height:2.25em"/></span>, no matter how we select the locations in <em class="EmphasisTypeItalic ">X</em>. A strategy to select locations in <em class="EmphasisTypeItalic ">X</em> is formalized by a <em class="EmphasisTypeItalic ">selection function</em>, and hence this notion says that no selection function should be able to give us an edge in predicting values from <em class="EmphasisTypeItalic ">X</em>. However, as in the above discussion on absolute randomness, in this full generality, <em class="EmphasisTypeItalic ">this notion is vacuous!</em> To counter this, von Mises proposed to restrict attention to “acceptable” selection rules, without further specifying which these should be. He called the sequences satisfying his requirement for randomness <em class="EmphasisTypeItalic ">Kollektiv</em>’s.<sup><a epub:type="noteref" href="#Fn4" id="Fn4_source">4</a></sup>
</div><div class="Para" id="Par16">Later Wald (<span class="CitationRef"><a epub:type="noteref" href="#CR29">1936</a></span>, <span class="CitationRef"><a epub:type="noteref" href="#CR30">1937</a></span>) showed that von Mises’ notion of Kollektiv is nonempty if we restrict to any <em class="EmphasisTypeItalic ">countable</em> set of selection functions.<sup><a epub:type="noteref" href="#Fn5" id="Fn5_source">5</a></sup> Wald did not specify a canonical choice for such a set, but later Church (<span class="CitationRef"><a epub:type="noteref" href="#CR6">1940</a></span>) suggested that the (countable) set of <em class="EmphasisTypeItalic ">computable</em> selection rules would be such a canonical choice. We thus arrive at the notion of <em class="EmphasisTypeItalic ">Mises–Wald–Church randomness</em>, defined as the set of Kollektiv’s based on computable selection rules. This notion of random sequence already contains several of the key ingredients of the modern theory of randomness, namely:<div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par18">the insight that randomness is a <em class="EmphasisTypeItalic ">relative</em> notion, not an absolute one, in that it depends on the choice of the set of selection rules;</p></li><li><p class="Para" id="Par19">it is founded on the theory of computation, by restricting attention to the <em class="EmphasisTypeItalic ">computable</em> selection functions (cf. Sect. <span class="InternalRef"><a href="#Sec4">4</a></span>).</p></li></ul></div>
Ville (<span class="CitationRef"><a epub:type="noteref" href="#CR27">1939</a></span>) later showed that von Mises’ notion of Kollektiv is flawed in the sense that there are basic statistical laws that are not satisfied by them. Nevertheless, the notion of Mises–Wald–Church randomness has been decisive for the subsequent developments in the theory of randomness.<sup><a epub:type="noteref" href="#Fn6" id="Fn6_source">6</a></sup>
</div><p class="Para" id="Par21">The Mises–Wald–Church notion formalized the intuition that a random sequence should be <em class="EmphasisTypeItalic ">unpredictable</em>. This was taken further by Ville using the notion of martingale. We discuss this approach in Sect. <span class="InternalRef"><a href="#Sec7">7</a></span>. The approach using Kolmogorov complexity formalizes the intuition that a random sequence, since it is lacking in recognizable structure, is hard to <em class="EmphasisTypeItalic ">describe</em>. We discuss this approach in Sect. <span class="InternalRef"><a href="#Sec5">5</a></span>. Finally, the notion randomness proposed by Martin-Löf formalizes the intuitions underlying classical probability and measure theory. This is discussed in Sect. <span class="InternalRef"><a href="#Sec6">6</a></span>. It is a highly remarkable fact that these approaches are intimately related, and ultimately turn out to be essentially equivalent. As the theory of computation is an essential ingredient in all of this, we have to briefly discuss it before we can proceed.</p></section><section class="Section1 RenderAsSection1" id="Sec4"><h2 class="Heading"><span class="HeadingNumber">4 </span>Computability Theory</h2><p class="Para" id="Par22">The theory of computation arose in the 1930s out of concerns about what is provable in mathematics and what is not. Gödel’s famous incompleteness theorem from 1931 states, informally speaking, that in any formal system strong enough to reason about arithmetic, <em class="EmphasisTypeItalic ">there always exist true statements that are not provable in the system</em>. This shows that there can never be a definitive formal system encompassing all of mathematics. Although it is a statement about mathematical provability, the proof of the incompleteness theorem shows that it is in essence a result about <em class="EmphasisTypeItalic ">computability</em>. The recursive functions used by Gödel in his proof of the incompleteness theorem were later shown by Turing (<span class="CitationRef"><a epub:type="noteref" href="#CR24">1936</a></span>) to define the same class of functions computable by a Turing machine. Subsequently, many equivalent definitions of the same class of computable functions were found, leading to a robust foundation for a general theory of computation, called <em class="EmphasisTypeItalic ">recursion theory</em>, referring to the recursive functions in Gödel’s proof. Nowadays the area is mostly called <em class="EmphasisTypeItalic ">computability theory</em>, to emphasize that it is about what is computable and what is not, rather than about recursion.</p><p class="Para" id="Par23">Turing machines serve as a very basic model of computation, which are nevertheless able to perform any type of algorithmic computation.<sup><a epub:type="noteref" href="#Fn7" id="Fn7_source">7</a></sup> The fortunate circumstance that there are so many equivalent definitions of the same class of computable functions allows us to treat this notion very informally, without giving a precise definition of what a Turing machine is. Thus, a <em class="EmphasisTypeItalic ">computable function</em> is a function for which there is an algorithm, i.e. a finite step-by-step procedure, that computes it. It is an empirical fact that any reasonable formalization of this concept leads to the same class of functions.<sup><a epub:type="noteref" href="#Fn8" id="Fn8_source">8</a></sup>
</p><p class="Para" id="Par26">Having a precise mathematical definition of the notion of computability allows us to prove that certain functions or problems are <em class="EmphasisTypeItalic ">not</em> computable. One of the most famous examples is Turing’s Halting Problem:</p><div class="FormalPara FormalParaRenderingStyle1" id="FPar1"><h3 class="Heading">
                <strong class="EmphasisTypeBold ">Definition 4.1</strong>
              </h3><p class="Para" id="Par27">The <em class="EmphasisTypeItalic ">Halting Problem</em> is the problem, given a Turing machine <em class="EmphasisTypeItalic ">M</em> and an input <em class="EmphasisTypeItalic ">x</em>, to decide whether <em class="EmphasisTypeItalic ">M</em> produces an output on <em class="EmphasisTypeItalic ">x</em> in a finite number of steps (as opposed to continuing indefinitely).</p></div><p class="Para" id="Par28">
Turing (<span class="CitationRef"><a epub:type="noteref" href="#CR24">1936</a></span>) showed that the Halting Problem is <em class="EmphasisTypeItalic ">undecidable</em>, that is, that there is no algorithm deciding it. (Note the self-referential flavor of this statement: There is no algorithm deciding the behavior of algorithms.) Not only does this point to a fundamental obstacle in computer science (which did not yet exist in at the time that Turing proved this result), but it also entails the undecidability of a host of other problems.<sup><a epub:type="noteref" href="#Fn9" id="Fn9_source">9</a></sup> Its importance for the theory of randomness will become clear in what follows.</p></section><section class="Section1 RenderAsSection1" id="Sec5"><h2 class="Heading"><span class="HeadingNumber">5 </span>Kolmogorov Complexity</h2><p class="Para" id="Par30">An old and venerable philosophical principle, called <em class="EmphasisTypeItalic ">Occam’s razor</em>, says that when given the choice between several hypotheses or explanations, one should always select the simplest one. The problem in applying this principle has always been to determine which is the simplest explanation: that which is simple in one context may be complicated in another, and there does not seem to be a canonical choice for a frame of reference.</p><p class="Para" id="Par31">A similar problem arises when we consider the two sequences on page 3: We would like to say that the first one, consisting of only 0’s, is simpler than the second, because it has a shorter <em class="EmphasisTypeItalic ">description</em>. But what are we to choose as our description mechanism? When we require, as seems reasonable, that an object can be effectively reconstructed from its description, the notion of Turing machine comes to mind. For simplicity we will for the moment only consider finite binary strings. (This is not a severe restriction, since many objects such as numbers and graphs can be <em class="EmphasisTypeItalic ">represented</em> as binary strings in a natural way.) Thus, given a Turing machine <em class="EmphasisTypeItalic ">M</em>, we define a string <em class="EmphasisTypeItalic ">y</em> to be a description of a string <em class="EmphasisTypeItalic ">x</em> if <span class="InlineEquation" id="IEq3"><img alt="$$M(y) = x$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq3.png" style="height:1.86em"/></span>, i.e. <em class="EmphasisTypeItalic ">M</em> produces <em class="EmphasisTypeItalic ">x</em> when given <em class="EmphasisTypeItalic ">y</em> as input. Now we can take the <em class="EmphasisTypeItalic ">length</em> of the string <em class="EmphasisTypeItalic ">y</em> as a measure of the complexity of <em class="EmphasisTypeItalic ">x</em>. However, this definition still depends on the choice of <em class="EmphasisTypeItalic ">M</em>. Kolmogorov observed that a canonical choice for <em class="EmphasisTypeItalic ">M</em> would be a <em class="EmphasisTypeItalic ">universal</em> Turing machine, that is, a machine that is able to simulate all other Turing machines. It is an elementary fact of computability theory that such universal machines exist. We thus arrive at the following definition:</p><div class="FormalPara FormalParaRenderingStyle1" id="FPar2"><h3 class="Heading">
                <strong class="EmphasisTypeBold ">Definition 5.1</strong>
              </h3><div class="Para" id="Par32">Fix a universal Turing machine <em class="EmphasisTypeItalic ">U</em>. The <em class="EmphasisTypeItalic ">Kolmogorov complexity</em> of of a finite binary string <em class="EmphasisTypeItalic ">x</em> is the smallest length of a string <em class="EmphasisTypeItalic ">y</em> such that<div class="Equation" id="Equ5"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$ U(y) = x. $$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_Equ5.png" style="height:1.86em"/></div></div></div></div>We denote the Kolmogorov complexity of the string <em class="EmphasisTypeItalic ">x</em> by <em class="EmphasisTypeItalic ">C</em>(<em class="EmphasisTypeItalic ">x</em>).</div></div><p class="Para" id="Par33">Hence, to say that <span class="InlineEquation" id="IEq4"><img alt="$$C(x)=n$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq4.png" style="height:1.86em"/></span> means that there is a string <em class="EmphasisTypeItalic ">y</em> of length <em class="EmphasisTypeItalic ">n</em> such that <span class="InlineEquation" id="IEq5"><img alt="$$U(y)=x$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq5.png" style="height:1.86em"/></span>, and that there is no such <em class="EmphasisTypeItalic ">y</em> of length smaller than <em class="EmphasisTypeItalic ">n</em>. Note that the definition of <em class="EmphasisTypeItalic ">C</em>(<em class="EmphasisTypeItalic ">x</em>) <em class="EmphasisTypeItalic ">still</em> depends on the choice of <em class="EmphasisTypeItalic ">U</em>. However, and this is the essential point, <em class="EmphasisTypeItalic ">the theory of Kolmogorov complexity is independent of the choice of U</em> in the sense that when we choose a different universal Turing machine <span class="InlineEquation" id="IEq6"><img alt="$$U^{\prime }$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq6.png" style="height:1.89em"/></span> as our frame of reference, the whole theory only shifts by a fixed constant.<sup><a epub:type="noteref" href="#Fn10" id="Fn10_source">10</a></sup> For this reason, the reference to <em class="EmphasisTypeItalic ">U</em> is suppressed from this point onwards, and we will simply speak about <em class="EmphasisTypeItalic ">the</em> Kolmogorov complexity of a string.</p><p class="Para" id="Par35">Armed with this definition of descriptive complexity, we can now define what it means for a finite string to be random. The idea is that a string is random if it has no description that is shorter than the string itself, that is, if there is no way to describe the string more efficiently than by listing it completely.</p><div class="FormalPara FormalParaRenderingStyle1" id="FPar3"><h3 class="Heading">
                <strong class="EmphasisTypeBold ">Definition 5.2</strong>
              </h3><p class="Para" id="Par36">A finite string <em class="EmphasisTypeItalic ">x</em> is <em class="EmphasisTypeItalic ">Kolmogorov random</em> if <em class="EmphasisTypeItalic ">C</em>(<em class="EmphasisTypeItalic ">x</em>) is at least the length of <em class="EmphasisTypeItalic ">x</em> itself.</p></div><p class="Para" id="Par37">For example, a sequence of 1000 zero’s is far from random, since its shortest description is much shorter than the string itself: The string itself has length 1000, but we have just described it using only a few words.<sup><a epub:type="noteref" href="#Fn11" id="Fn11_source">11</a></sup> More generally, if a string contains a regular pattern that can be used to efficiently describe it, then it is not random. Thus this notion of randomness is related to the <em class="EmphasisTypeItalic ">compression</em> of strings: If <span class="InlineEquation" id="IEq9"><img alt="$$U(y)=x$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq9.png" style="height:1.86em"/></span>, and <em class="EmphasisTypeItalic ">y</em> is shorter than <em class="EmphasisTypeItalic ">x</em>, we may think of <em class="EmphasisTypeItalic ">y</em> as a <em class="EmphasisTypeItalic ">compressed</em> version of <em class="EmphasisTypeItalic ">x</em>, and random strings are those that cannot be compressed.</p><p class="Para" id="Par39">A major hindrance in using Kolmogorov complexity is the fact that <em class="EmphasisTypeItalic ">the complexity function C is noncomputable</em>. A precise proof of this fact is given in Appendix B (see Corollary <span class="InternalRef"><a href="#FPar8">B.2</a></span>), but it is also intuitively plausible, since to compute the complexity of <em class="EmphasisTypeItalic ">y</em> we have to see for which inputs <em class="EmphasisTypeItalic ">x</em> the universal machine <em class="EmphasisTypeItalic ">U</em> produces <em class="EmphasisTypeItalic ">y</em> as output. But as we have seen in Sect. <span class="InternalRef"><a href="#Sec4">4</a></span>, this is in general impossible to do by the undecidability of the Halting Problem! This leaves us with a definition that may be wonderful for theoretical purposes, but that one would not expect to be of much practical relevance. One of the miracles of Kolmogorov complexity is that the subject <em class="EmphasisTypeItalic ">does</em> indeed have genuine applications, many of which are discussed in the book by Li and Vitányi (<span class="CitationRef"><a epub:type="noteref" href="#CR16">2008</a></span>). We will briefly discuss applications in Sect. <span class="InternalRef"><a href="#Sec11">11</a></span>.</p><p class="Para" id="Par40">We will not go into the delicate subject of the history of Kolmogorov complexity, other than saying that it was invented by Solomonoff, Kolmogorov, and Chaitin (in that order), and we refer to Li and Vitányi (<span class="CitationRef"><a epub:type="noteref" href="#CR16">2008</a></span>) and Downey and Hirschfeldt (<span class="CitationRef"><a epub:type="noteref" href="#CR8">2010</a></span>) for further information.</p></section><section class="Section1 RenderAsSection1" id="Sec6"><h2 class="Heading"><span class="HeadingNumber">6 </span>Martin-Löf Randomness</h2><p class="Para" id="Par41">The notion of Martin-Löf randomness, introduced by Martin-Löf in (<span class="CitationRef"><a epub:type="noteref" href="#CR18">1966</a></span>), is based on classical probability theory, which in its modern formulation is phrased in terms of <em class="EmphasisTypeItalic ">measure theory</em>. In Appendix A the notion of a measure space is explained in some detail, but for now we keep the discussion as light as possible.</p><p class="Para" id="Par42">The unit interval [0, 1] consists of all the numbers on the real line between 0 and 1. We wish to discuss probabilities in this setting by assigning to subsets <em class="EmphasisTypeItalic ">A</em> of the unit interval, called <em class="EmphasisTypeItalic ">events</em>, a probability, which informally should be the probability that when we “randomly” pick a real from [0, 1] that we end up in <em class="EmphasisTypeItalic ">A</em>. The <em class="EmphasisTypeItalic ">uniform</em> or <em class="EmphasisTypeItalic ">Lebesgue measure</em> on [0, 1] assigns the measure <span class="InlineEquation" id="IEq10"><img alt="$$b-a$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq10.png" style="height:1.86em"/></span> to every interval [<em class="EmphasisTypeItalic ">a</em>, <em class="EmphasisTypeItalic ">b</em>], i.e. the measure of an interval is simply its length. For example, the interval <span class="InlineEquation" id="IEq11"><img alt="$$[0,\frac{1}{2}]$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq11.png" style="height:2.25em"/></span> has measure <span class="InlineEquation" id="IEq12"><img alt="$$\frac{1}{2}$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq12.png" style="height:2.25em"/></span>, the interval <span class="InlineEquation" id="IEq13"><img alt="$$[\frac{3}{4},1]$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq13.png" style="height:2.25em"/></span> has measure <span class="InlineEquation" id="IEq14"><img alt="$$\frac{1}{4}$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq14.png" style="height:2.25em"/></span>. Note that [0, 1] itself has measure 1.</p><p class="Para" id="Par43">Given this, we can also define the measure of more complicated sets by considering combinations of intervals. For example, we give the combined event consisting of the union of the intervals <span class="InlineEquation" id="IEq15"><img alt="$$[0,\frac{1}{2}]$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq15.png" style="height:2.25em"/></span> and <span class="InlineEquation" id="IEq16"><img alt="$$[\frac{3}{4},1]$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq16.png" style="height:2.25em"/></span> the measure <span class="InlineEquation" id="IEq17"><img alt="$$\frac{1}{2} + \frac{1}{4} = \frac{3}{4}$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq17.png" style="height:2.25em"/></span>. Since the measures of the subsets of [0, 1] defined in this way satisfy the laws of probability (cf. Appendix A), we can think of them as <em class="EmphasisTypeItalic ">probabilities</em>.</p><p class="Para" id="Par44">A series of intervals is called a <em class="EmphasisTypeItalic ">cover</em> for an event <em class="EmphasisTypeItalic ">A</em> if <em class="EmphasisTypeItalic ">A</em> is contained in the union of all the intervals in the series. Now an event <em class="EmphasisTypeItalic ">A</em> is defined to have measure 0 if it is possible to cover <em class="EmphasisTypeItalic ">A</em> with intervals in such a way that the total sum of the lengths of all the intervals can be chosen arbitrarily small.</p><p class="Para" id="Par45">For example, for every real <em class="EmphasisTypeItalic ">x</em> in [0, 1], the event <em class="EmphasisTypeItalic ">A</em> consisting only of the real <em class="EmphasisTypeItalic ">x</em> has measure 0, since for every <em class="EmphasisTypeItalic ">n</em>, <em class="EmphasisTypeItalic ">x</em> is contained in the interval <span class="InlineEquation" id="IEq18"><img alt="$$[x-\frac{1}{n},x+\frac{1}{n}]$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq18.png" style="height:2.25em"/></span>, and the length of the latter interval is <span class="InlineEquation" id="IEq19"><img alt="$$2\frac{1}{n}$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq19.png" style="height:2.25em"/></span>, which tends to 0 if <em class="EmphasisTypeItalic ">n</em> tends to infinity.</p><p class="Para" id="Par46">These definitions suffice to do probability theory on [0, 1], and to speak informally about picking reals “at random”, but we now wish to define what it means for a <em class="EmphasisTypeItalic ">single</em> real <em class="EmphasisTypeItalic ">x</em> to be random. We can view any event of measure 0 as a “test for randomness”, where the elements not included in the event pass the test, and those in it fail. All the usual statistical laws, such as the law of large numbers, correspond to such tests. Now we would like to define <em class="EmphasisTypeItalic ">x</em> to be random if <em class="EmphasisTypeItalic ">x</em> passes all statistical tests, i.e. <em class="EmphasisTypeItalic ">x</em> is not in any set of measure 0. But, as we have just seen in the example above, every single real <em class="EmphasisTypeItalic ">x</em> has measure 0, hence in its full generality this definition is vacuous. (The reader may compare this to the situation we already encountered above in Sect. <span class="InternalRef"><a href="#Sec3">3</a></span> when we discussed Kollektiv’s.)</p><p class="Para" id="Par47">However, as Martin-Löf observed, we obtain a viable definition if we restrict ourselves to a countable collection of measure 0 sets. More precisely, let us say that an event <em class="EmphasisTypeItalic ">A</em> has <em class="EmphasisTypeItalic ">effective measure 0</em> if there is a <em class="EmphasisTypeItalic ">computable</em> series of covers of <em class="EmphasisTypeItalic ">A</em>, with the measure of the covers in the series tending to 0. Phrased more informally: <em class="EmphasisTypeItalic ">A</em> has effective measure 0 if there is an <em class="EmphasisTypeItalic ">algorithm</em> witnessing that <em class="EmphasisTypeItalic ">A</em> has measure 0, by producing an appropriate series of covers for <em class="EmphasisTypeItalic ">A</em>. Now we can finally define:</p><div class="FormalPara FormalParaRenderingStyle1" id="FPar4"><h3 class="Heading">
                <strong class="EmphasisTypeBold ">Definition 6.1</strong>
              </h3><p class="Para" id="Par48">A real <em class="EmphasisTypeItalic ">x</em> is <em class="EmphasisTypeItalic ">Martin-Löf random</em> if <em class="EmphasisTypeItalic ">x</em> is not contained in any event of effective measure 0.</p></div><p class="Para" id="Par49">It can be shown that with this modification random reals exist.<sup><a epub:type="noteref" href="#Fn12" id="Fn12_source">12</a></sup> Moreover, <em class="EmphasisTypeItalic ">almost every real in [0, 1] is random</em>, in the sense that the set of nonrandom reals is of effective measure 0.</p><p class="Para" id="Par51">Note the analogy between Definition <span class="InternalRef"><a href="#FPar4">6.1</a></span> and the way that Church modified von Mises definition of Kollektiv, as described in Sect. <span class="InternalRef"><a href="#Sec3">3</a></span>: There we restricted to the computable selection functions, here we restrict to the effective measure 0 events.</p><p class="Para" id="Par52">Identifying a real number <em class="EmphasisTypeItalic ">x</em> with its decimal expansion,<sup><a epub:type="noteref" href="#Fn13" id="Fn13_source">13</a></sup> we have thus obtained a definition of randomness for infinite sequences. The question now immediately presents itself what the relation, if any, of this definition is with the definition of randomness of <em class="EmphasisTypeItalic ">finite</em> sequences from Sect. <span class="InternalRef"><a href="#Sec5">5</a></span>. A first guess could be that an infinite sequence is random in the sense of Martin-Löf if and only if all of its finite initial segments are random in the sense of Kolmogorov, but this turns out to be false. A technical modification to Definition <span class="InternalRef"><a href="#FPar2">5.1</a></span> is needed to make this work.</p><p class="Para" id="Par54">A string <em class="EmphasisTypeItalic ">y</em> is called a <em class="EmphasisTypeItalic ">prefix</em> of a string <span class="InlineEquation" id="IEq21"><img alt="$$y'$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq21.png" style="height:1.89em"/></span> if <em class="EmphasisTypeItalic ">y</em> is an initial segment of <span class="InlineEquation" id="IEq22"><img alt="$$y'$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq22.png" style="height:1.89em"/></span>. For example, the string 001 is a prefix of the string 001101. Let us now impose the following restriction on descriptions: If <span class="InlineEquation" id="IEq23"><img alt="$$U(y) = x$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq23.png" style="height:1.86em"/></span>, i.e. <em class="EmphasisTypeItalic ">y</em> is a description of <em class="EmphasisTypeItalic ">x</em>, and <span class="InlineEquation" id="IEq24"><img alt="$$U(y')=x'$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq24.png" style="height:1.89em"/></span>, then we require that <em class="EmphasisTypeItalic ">y</em> is not a prefix of <span class="InlineEquation" id="IEq25"><img alt="$$y'$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq25.png" style="height:1.89em"/></span>. This restriction may seem arbitrary, but we can motivate it as follows. Suppose that we identify persons by their phone numbers. It is then a natural restriction that no phone number is a prefix of another, since if the phone number <em class="EmphasisTypeItalic ">y</em> of <em class="EmphasisTypeItalic ">x</em> were a prefix of a phone number <span class="InlineEquation" id="IEq26"><img alt="$$y'$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq26.png" style="height:1.89em"/></span> of <span class="InlineEquation" id="IEq27"><img alt="$$x'$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq27.png" style="height:1.89em"/></span>, then when trying to call <span class="InlineEquation" id="IEq28"><img alt="$$x'$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq28.png" style="height:1.89em"/></span> we would end up talking to <em class="EmphasisTypeItalic ">x</em>. Indeed, in practice phone numbers are not prefixes of one another. We say that the set of phone numbers is <em class="EmphasisTypeItalic ">prefix-free</em>. We now require that the set of descriptions <em class="EmphasisTypeItalic ">y</em> used as inputs for the universal machine <em class="EmphasisTypeItalic ">U</em> in Definition <span class="InternalRef"><a href="#FPar2">5.1</a></span> is prefix-free. Of course, this changes the definition of the complexity function <em class="EmphasisTypeItalic ">C</em>(<em class="EmphasisTypeItalic ">x</em>): Since there are fewer descriptions available, in general the descriptive complexity of strings will be higher. The complexity of strings under this new definition is called the <em class="EmphasisTypeItalic ">prefix-free complexity</em>. The underlying idea of the prefix-free complexity is the same as that of Kolmogorov complexity, but technically the theory of it differs from Kolmogorov complexity in several important ways. For us, at this point of the discussion, the most important feature of it is the following landmark result. It was proven in 1973 by Claus-Peter Schnorr, one of the pioneers of the subject.</p><div class="FormalPara FormalParaRenderingStyle1" id="FPar5"><h3 class="Heading">
                <strong class="EmphasisTypeBold ">Theorem 6.2</strong>
              </h3><p class="Para" id="Par55">(Schnorr <span class="CitationRef"><a epub:type="noteref" href="#CR22">1973</a></span>) An infinite sequence <em class="EmphasisTypeItalic ">X</em> is Martin-Löf random if and only if there is a constant <em class="EmphasisTypeItalic ">c</em> such that every initial segment of <em class="EmphasisTypeItalic ">X</em> of length <em class="EmphasisTypeItalic ">n</em> has prefix-free complexity at least <span class="InlineEquation" id="IEq29"><img alt="$$n-c$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq29.png" style="height:1.86em"/></span>.</p></div><p class="Para" id="Par56">The reader should take a moment to let the full meaning and beauty of this theorem sink in. It offers no less than an equivalence between two seemingly unrelated theories. One is the theory of randomness for finite sequences, based on descriptive complexity, and the other is the theory of infinite sequences, based on measure theory. The fact that there is a relation between these theories at all is truly remarkable.</p></section><section class="Section1 RenderAsSection1" id="Sec7"><h2 class="Heading"><span class="HeadingNumber">7 </span>Martingales</h2><div class="Para" id="Par57">Thus far we have seen three different formalizations of intuitions underlying randomness:<div class="OrderedList"><ol><li class="ListItem"><span class="ItemNumber">(i)</span><div class="ItemContent"><p class="Para" id="Par58">Mises–Wald–Church randomness, formalizing unpredictability using selection functions,</p></div><div class="ClearBoth"> </div></li><li class="ListItem"><span class="ItemNumber">(ii)</span><div class="ItemContent"><p class="Para" id="Par59">Kolmogorov complexity, based on descriptive complexity,</p></div><div class="ClearBoth"> </div></li><li class="ListItem"><span class="ItemNumber">(iii)</span><div class="ItemContent"><p class="Para" id="Par60">Martin-Löf randomness, based on measure theory.</p></div><div class="ClearBoth"> </div></li></ol></div>
</div><div class="Para" id="Par61">Theorem <span class="InternalRef"><a href="#FPar5">6.2</a></span> provided the link between (ii) and (iii), and (i) was discussed in Sect. <span class="InternalRef"><a href="#Sec3">3</a></span>. We already mentioned Ville, who showed that the notion in (i) was flawed in a certain sense. Ville also showed an alternative way to formalize the notion of unpredictability of an infinite sequence, using the notion of a <em class="EmphasisTypeItalic ">martingale</em>, which we now discuss.<sup><a epub:type="noteref" href="#Fn14" id="Fn14_source">14</a></sup> Continuing our game-theoretic discussion of Sect. <span class="InternalRef"><a href="#Sec3">3</a></span>, we imagine that we are playing against an unknown infinite binary sequence <em class="EmphasisTypeItalic ">X</em>. At each stage of the game, we are shown a finite initial part<div class="Equation" id="Equ6"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$ X(0), X(1), X(2), \ldots , X(n-1) $$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_Equ6.png" style="height:1.86em"/></div></div></div></div>of the sequence <em class="EmphasisTypeItalic ">X</em>, and we are asked to bet on the next value <em class="EmphasisTypeItalic ">X</em>(<em class="EmphasisTypeItalic ">n</em>). Suppose that at this stage of the game, we have a capital of <em class="EmphasisTypeItalic ">d</em> dollar. Now we may split the amount <em class="EmphasisTypeItalic ">d</em> into parts <span class="InlineEquation" id="IEq30"><img alt="$$b_0$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq30.png" style="height:1.86em"/></span> and <span class="InlineEquation" id="IEq31"><img alt="$$b_1$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq31.png" style="height:1.86em"/></span>, and bet the amount <span class="InlineEquation" id="IEq32"><img alt="$$b_0$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq32.png" style="height:1.86em"/></span> that <em class="EmphasisTypeItalic ">X</em>(<em class="EmphasisTypeItalic ">n</em>) is 0, and the amount <span class="InlineEquation" id="IEq33"><img alt="$$b_1$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq33.png" style="height:1.86em"/></span> that <em class="EmphasisTypeItalic ">X</em>(<em class="EmphasisTypeItalic ">n</em>) is 1. After placing our bets, we receive a payoff <span class="InlineEquation" id="IEq34"><img alt="$$d_0 = 2b_0$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq34.png" style="height:1.86em"/></span> if <span class="InlineEquation" id="IEq35"><img alt="$$X(n)=0$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq35.png" style="height:1.86em"/></span>, and a payoff <span class="InlineEquation" id="IEq36"><img alt="$$d_1 = 2b_1$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq36.png" style="height:1.86em"/></span> if <span class="InlineEquation" id="IEq37"><img alt="$$X(n)=1$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq37.png" style="height:1.86em"/></span>. Hence the payoffs satisfy the equation<div class="Equation NumberedEquation" id="Equ1"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\begin{aligned} \frac{d_0 + d_1}{2} = d. \end{aligned}$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_Equ1.png" style="height:3.15em"/></div></div> <div class="EquationNumber">(1)</div></div></div>After placing our bets, we receive a payoff <span class="InlineEquation" id="IEq38"><img alt="$$d_0$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq38.png" style="height:1.86em"/></span> if <span class="InlineEquation" id="IEq39"><img alt="$$X(n)\,{=}\,0$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq39.png" style="height:1.86em"/></span>, and a payoff <span class="InlineEquation" id="IEq40"><img alt="$$d_1$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq40.png" style="height:1.86em"/></span> if <span class="InlineEquation" id="IEq41"><img alt="$$X(n)\,{=}\,1$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq41.png" style="height:1.86em"/></span>.</div><p class="Para" id="Par63">For example, we may let <span class="InlineEquation" id="IEq42"><img alt="$$b_0 = b_1 = \frac{1}{2}d$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq42.png" style="height:2.25em"/></span>, in which case our payoff will be <em class="EmphasisTypeItalic ">d</em>, no matter what <em class="EmphasisTypeItalic ">X</em>(<em class="EmphasisTypeItalic ">n</em>) is. So this is the same as not betting at all, and leaving our capital intact. But we can also set <span class="InlineEquation" id="IEq43"><img alt="$$b_0 = d$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq43.png" style="height:1.86em"/></span> and <span class="InlineEquation" id="IEq44"><img alt="$$b_1 =0$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq44.png" style="height:1.86em"/></span>. In this case, if <span class="InlineEquation" id="IEq45"><img alt="$$X(n)=0$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq45.png" style="height:1.86em"/></span> we receive a payoff of 2<em class="EmphasisTypeItalic ">d</em>, and we have doubled our capital. However, if it turns out that <span class="InlineEquation" id="IEq46"><img alt="$$X(n)=1$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq46.png" style="height:1.86em"/></span>, we receive 0, and we have lost everything. Hence this placement of the bets should be made only when we are quite sure that <span class="InlineEquation" id="IEq47"><img alt="$$X(n)=0$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq47.png" style="height:1.86em"/></span>. Any other placement of bets between these two extremes can be made, reflecting our willingness to bet on <span class="InlineEquation" id="IEq48"><img alt="$$X(n)=0$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq48.png" style="height:1.86em"/></span> or <span class="InlineEquation" id="IEq49"><img alt="$$X(n)=1$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq49.png" style="height:1.86em"/></span>.</p><p class="Para" id="Par64">After betting on <em class="EmphasisTypeItalic ">X</em>(<em class="EmphasisTypeItalic ">n</em>), the value <em class="EmphasisTypeItalic ">X</em>(<em class="EmphasisTypeItalic ">n</em>) is revealed, we receive our payoff for this round, and the game continues with betting on <span class="InlineEquation" id="IEq50"><img alt="$$X(n+1)$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq50.png" style="height:1.86em"/></span>.</p><div class="Para" id="Par65">Now the idea of Ville’s definition is that we should not be able to win an infinite amount of money by betting on a random sequence. For a given binary string <span class="InlineEquation" id="IEq51"><img alt="$$\sigma $$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq51.png" style="height:1.86em"/></span>, let <span class="InlineEquation" id="IEq52"><img alt="$$\sigma \widehat{{}}0$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq52.png" style="height:2em"/></span> denote the string <span class="InlineEquation" id="IEq53"><img alt="$$\sigma $$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq53.png" style="height:1.86em"/></span> extended by a 0, and <span class="InlineEquation" id="IEq54"><img alt="$$\sigma \widehat{{}}1$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq54.png" style="height:2em"/></span> the string <span class="InlineEquation" id="IEq55"><img alt="$$\sigma $$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq55.png" style="height:1.86em"/></span> extended by a 1. Formally, a <em class="EmphasisTypeItalic ">martingale</em> is a function <em class="EmphasisTypeItalic ">d</em> such that for every finite string <span class="InlineEquation" id="IEq56"><img alt="$$\sigma $$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq56.png" style="height:1.86em"/></span> the martingale equality<div class="Equation NumberedEquation" id="Equ2"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\begin{aligned} \frac{d(\sigma \widehat{{}}0) + d(\sigma \widehat{{}}1)}{2} = d(\sigma ) \end{aligned}$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_Equ2.png" style="height:3.22em"/></div></div> <div class="EquationNumber">(2)</div></div></div>holds. The meaning of this equation is that when we are seeing the initial segment <span class="InlineEquation" id="IEq57"><img alt="$$\sigma $$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq57.png" style="height:1.86em"/></span>, and we have a capital <span class="InlineEquation" id="IEq58"><img alt="$$d(\sigma )$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq58.png" style="height:1.86em"/></span>, we can bet the amount <span class="InlineEquation" id="IEq59"><img alt="$$\frac{1}{2}d(\sigma \widehat{{}}0)$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq59.png" style="height:2.25em"/></span> that the next value will be a zero, and <span class="InlineEquation" id="IEq60"><img alt="$$\frac{1}{2}d(\sigma \widehat{{}}1)$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq60.png" style="height:2.25em"/></span> that the next value will be a one, just as above in Eq. (<span class="InternalRef"><a href="#Equ1">1</a></span>). Thus the martingale <em class="EmphasisTypeItalic ">d</em> represents a particular <em class="EmphasisTypeItalic ">betting strategy</em>. Now for a random sequence <em class="EmphasisTypeItalic ">X</em>, the amounts of capital<div class="Equation" id="Equ7"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$ d\big (X(0),\ldots , X(n-1)\big ) $$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_Equ7.png" style="height:2.04em"/></div></div></div></div>that we win when betting on <em class="EmphasisTypeItalic ">X</em> should not tend to infinity.<sup><a epub:type="noteref" href="#Fn15" id="Fn15_source">15</a></sup>
</div><p class="Para" id="Par67">As in the case of Mises–Wald–Church randomness and the case of Martin-Löf randomness, this definition only makes sense when we restrict ourselves to a countable class of martingales.<sup><a epub:type="noteref" href="#Fn16" id="Fn16_source">16</a></sup> A natural choice would be to consider the <em class="EmphasisTypeItalic ">computable</em> martingales. The resulting notion of randomness was studied in Schnorr (<span class="CitationRef"><a epub:type="noteref" href="#CR21">1971</a></span>), and it turns out to be <em class="EmphasisTypeItalic ">weaker</em> than Martin-Löf randomness. However, <em class="EmphasisTypeItalic ">there exists another natural class of martingales, the so-called</em> <em class="EmphasisTypeItalic ">c.e.-martingales</em>,<sup><a epub:type="noteref" href="#Fn17" id="Fn17_source">17</a></sup> <em class="EmphasisTypeItalic ">such that the resulting notion of randomness is equivalent to Martin-Löf randomness.</em></p><p class="Para" id="Par70">Thus Ville’s approach to formalizing the notion of unpredictability using martingales gives yet a third equivalent way to define the same notion of randomness.</p></section><section class="Section1 RenderAsSection1" id="Sec8"><h2 class="Heading"><span class="HeadingNumber">8 </span>Randomness and Provability</h2><div class="Para" id="Par71">By Gödel’s incompleteness theorem (see Sect. <span class="InternalRef"><a href="#Sec4">4</a></span>), in any reasonable formal system of arithmetic, there exist formulas that are true yet unprovable. A consequence of this result is that there is no algorithm to decide the truth of arithmetical formulas. It follows from the undecidability of the Halting Problem (see Definition <span class="InternalRef"><a href="#FPar1">4.1</a></span>) that the set of formulas that are <em class="EmphasisTypeItalic ">provable</em> is also undecidable.<sup><a epub:type="noteref" href="#Fn18" id="Fn18_source">18</a></sup> However, the set of provable formulas is <em class="EmphasisTypeItalic ">computably enumerable</em>, meaning that there is an algorithm that lists all the provable statements. Computably enumerable, or c.e., sets, play an important role in computability theory. For example, the set <em class="EmphasisTypeItalic ">H</em> representing the Halting Problem is an example of a c.e. set, because we can in principle make an infinite list of all the halting computations.<sup><a epub:type="noteref" href="#Fn19" id="Fn19_source">19</a></sup> The complement <span class="InlineEquation" id="IEq63"><img alt="$$\overline{H}$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq63.png" style="height:1.23em"/></span> of the set <em class="EmphasisTypeItalic ">H</em>, consisting of all nonconvergent computations, is <em class="EmphasisTypeItalic ">not</em> c.e. For if it were, we could decide membership in <em class="EmphasisTypeItalic ">H</em> as follows: Given a pair <em class="EmphasisTypeItalic ">M</em> and <em class="EmphasisTypeItalic ">x</em>, effectively list both <em class="EmphasisTypeItalic ">H</em> and its complement <span class="InlineEquation" id="IEq64"><img alt="$$\overline{H}$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq64.png" style="height:1.23em"/></span> until the pair appears in one of them, thus answering the question whether the computation <em class="EmphasisTypeItalic ">M</em>(<em class="EmphasisTypeItalic ">x</em>) converges. Since <em class="EmphasisTypeItalic ">H</em> is not computable, it follows that <span class="InlineEquation" id="IEq65"><img alt="$$\overline{H}$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq65.png" style="height:1.23em"/></span> cannot be c.e. Because the set of all provable statements is c.e., it also follows that not all statements of the form<div class="Equation" id="Equ8"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$ ``M(x)\,\text {does not halt}\text {''} $$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_Equ8.png" style="height:1.86em"/></div></div></div></div>are provable. Hence there exist computations that do not halt, but for which this fact is not provable! Thus we obtain a specific example of a true, but unprovable statement. The same kind of reasoning applies if we replace <em class="EmphasisTypeItalic ">H</em> by any other noncomputable c.e. set.</div><div class="Para" id="Par74">Now consider the set <em class="EmphasisTypeItalic ">R</em> of all strings that are Kolmogorov random, and let non-<em class="EmphasisTypeItalic ">R</em> be the set of all strings that are not Kolmogorov random. We have the following facts:<div class="OrderedList"><ol><li class="ListItem"><span class="ItemNumber">(i)</span><div class="ItemContent"><p class="Para" id="Par75">non-<em class="EmphasisTypeItalic ">R</em> is c.e. This is easily seen as follows: If <em class="EmphasisTypeItalic ">x</em> is not random, there is a description <em class="EmphasisTypeItalic ">y</em> shorter than <em class="EmphasisTypeItalic ">x</em> such that <span class="InlineEquation" id="IEq66"><img alt="$$U(y)=x$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq66.png" style="height:1.86em"/></span>. Since the set of halting computations is c.e., it follows that non-<em class="EmphasisTypeItalic ">R</em> is also c.e.</p></div><div class="ClearBoth"> </div></li><li class="ListItem"><span class="ItemNumber">(ii)</span><div class="ItemContent"><p class="Para" id="Par76"><em class="EmphasisTypeItalic ">R</em> is not c.e. This is proved in Theorem <span class="InternalRef"><a href="#FPar6">B.1</a></span> in Appendix B.</p></div><div class="ClearBoth"> </div></li></ol></div>
</div><div class="Para" id="Par77">By applying the same reasoning as for <em class="EmphasisTypeItalic ">H</em> above, we conclude from this that there are statements of the form<div class="Equation" id="Equ9"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$ \text {``}x\;\text {is random''} $$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_Equ9.png" style="height:1.86em"/></div></div></div></div>that are true, but not provable. This is Chaitin’s version of the incompleteness theorem, cf. Chaitin (<span class="CitationRef"><a epub:type="noteref" href="#CR5">1974</a></span>).<sup><a epub:type="noteref" href="#Fn20" id="Fn20_source">20</a></sup>
</div></section><section class="Section1 RenderAsSection1" id="Sec9"><h2 class="Heading"><span class="HeadingNumber">9 </span>Other Notions of Randomness</h2><p class="Para" id="Par80">Mises–Wald–Church random sequences were defined using computable selection functions, and Martin-Löf random sequences with computable covers, which in Ville’s approach correspond to c.e.-martingales. As Wald already pointed out in the case of Kollektiv’s, all of these notions can be defined relative to any countable collection of selection functions, respectively covers and martingales. Choosing computable covers in the case of Martin-Löf randomness gave the fundamental and appealing connection with Kolmogorov randomness (Theorem <span class="InternalRef"><a href="#FPar5">6.2</a></span>), but there are situations in which this is either too weak, or too strong. Viewing the level of computability of covers and martingales as a parameter that we can vary allows us to introduce notions of randomness that are either weaker or stronger than the ones we have discussed so far.</p><p class="Para" id="Par81">In his groundbreaking book (<span class="CitationRef"><a epub:type="noteref" href="#CR21">1971</a></span>), Schnorr discussed alternatives to the notion of Martin-Löf randomness, thus challenging the status of this notion (not claimed by Martin-Löf himself) as the “true” notion of randomness.<sup><a epub:type="noteref" href="#Fn21" id="Fn21_source">21</a></sup>
</p><p class="Para" id="Par83">In studying the randomness notions corresponding to various levels of computability, rather than yielding a single “true” notion of randomness, a picture has emerged in which every notion has a corresponding context in which it fruitfully can be applied. This ranges from low levels of complexity in computational complexity theory (see e.g. the survey paper by Lutz <span class="CitationRef"><a epub:type="noteref" href="#CR17">1997</a></span>), to the levels of computability (computable and c.e.) that we have been discussing in the previous sections, to higher levels of computability, all the way up to the higher levels of set theory. In studying notions of randomness across these levels, randomness has also served as a unifying theme between various areas of mathematical logic.</p><p class="Para" id="Par84">The general theory also serves as a background for the study of specific cases. Consider the example of <span class="InlineEquation" id="IEq67"><img alt="$$\pi $$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq67.png" style="height:1.86em"/></span>. Since <span class="InlineEquation" id="IEq68"><img alt="$$\pi $$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq68.png" style="height:1.86em"/></span> is a computable real number, its decimal expansion is perfectly predictable, and hence <span class="InlineEquation" id="IEq69"><img alt="$$\pi $$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq69.png" style="height:1.86em"/></span> it is not random in any of the senses discussed above. However, the distribution of the digits <span class="InlineEquation" id="IEq70"><img alt="$$0,\ldots ,9$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq70.png" style="height:1.86em"/></span> in <span class="InlineEquation" id="IEq71"><img alt="$$\pi $$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq71.png" style="height:1.86em"/></span> appears to be “random”. Real numbers with a decimal expansion in which every digit occurs with frequency <span class="InlineEquation" id="IEq72"><img alt="$$\frac{1}{10}$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq72.png" style="height:2.25em"/></span>, and more general, every block of digits of length <em class="EmphasisTypeItalic ">n</em> occurs with frequency <span class="InlineEquation" id="IEq73"><img alt="$$\frac{1}{10^n}$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq73.png" style="height:2.25em"/></span>, are called <em class="EmphasisTypeItalic ">normal</em> to base 10. Normality can be seen as a very weak notion of randomness, where we consider just one type of statistical test, instead of infinitely many as in the case of Martin-Löf randomness. It is in fact not known if <span class="InlineEquation" id="IEq74"><img alt="$$\pi $$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq74.png" style="height:1.86em"/></span> is normal to base 10, but it is conjectured that <span class="InlineEquation" id="IEq75"><img alt="$$\pi $$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq75.png" style="height:1.86em"/></span> is indeed “random” in this weak sense. For a recent discussion of the notion of normality, see Becher and Slaman (<span class="CitationRef"><a epub:type="noteref" href="#CR3">2014</a></span>).</p></section><section class="Section1 RenderAsSection1" id="Sec10"><h2 class="Heading"><span class="HeadingNumber">10 </span>Pseudorandom Number Generators and Complexity Theory</h2><p class="Para" id="Par85">In many contexts, it is desirable to have a good source of random numbers, for example when one wants to take an unbiased random sample, in the simulation of economic or atmospheric models, or when using statistical methods to estimate things that are difficult to compute directly (the so-called Monte Carlo method). In such a case, one may turn to physical devices (which begs the question about randomness of physical sources), or one may try to generate random strings using a computer. However, the outcome of a deterministic procedure on a computer cannot be random in any of the senses discussed above. (By Theorem <span class="InternalRef"><a href="#FPar6">B.1</a></span> in Appendix B, there is no purely algorithmic way of effectively generating infinitely many random strings, and it is easy to see that a Martin-Löf random set cannot be computable.) Hence the best an algorithm can do is to produce an outcome that is <em class="EmphasisTypeItalic ">pseudorandom</em>, that is, “random enough”, where the precise meaning of “random enough” depends on the context. In practice this usually means that the outcome should pass a number of standard statistical tests. Such procedures are called <em class="EmphasisTypeItalic ">pseudorandom number generators</em>. That the outcomes of a pseudorandom number generator should not be taken as truly random was pointed out by the great mathematician and physicist John von Neumann, when he remarked that</p><div class="Para" id="Par86">
              <blockquote class="BlockQuote"><p class="Para" id="Par87"> Anyone who considers arithmetical methods of producing random digits is, of course, in a state of sin.<sup><a epub:type="noteref" href="#Fn22" id="Fn22_source">22</a></sup>
</p></blockquote>
            </div><p class="Para" id="Par89"><em class="EmphasisTypeItalic ">Randomized algorithms</em> are algorithms that employ randomness during computations, and that allow for a small probability of error in their answers. For example, the first feasible<sup><a epub:type="noteref" href="#Fn23" id="Fn23_source">23</a></sup> algorithms to determine whether a number is prime were randomized algorithms.<sup><a epub:type="noteref" href="#Fn24" id="Fn24_source">24</a></sup> An important theme in computational complexity theory is the extent to which it is possible to <em class="EmphasisTypeItalic ">derandomize</em> randomized algorithms, i.e. to convert them to deterministic algorithms. This is connected to fundamental open problems about the relation between deterministic algorithms, nondeterministic algorithms, and randomized computation.<sup><a epub:type="noteref" href="#Fn25" id="Fn25_source">25</a></sup> Besides being of theoretical interest, this matter is of great practical importance, for example in the security of cryptographic schemes that are currently widely used. For an overview of current research we refer the reader to Arora and Barak (<span class="CitationRef"><a epub:type="noteref" href="#CR2">2009</a></span>). It is also interesting to note that randomness plays an important part in many of the proofs of results about deterministic algorithms, that do not otherwise mention randomness.</p></section><section class="Section1 RenderAsSection1" id="Sec11"><h2 class="Heading"><span class="HeadingNumber">11 </span>Applications</h2><p class="Para" id="Par93">As pointed out in Sect. <span class="InternalRef"><a href="#Sec5">5</a></span> and Corollary <span class="InternalRef"><a href="#FPar8">B.2</a></span>, due to the undecidability of the Halting Problem, the notion of Kolmogorov complexity is inherently noncomputable. This means that there is no algorithm that, given a finite sequence, can compute its complexity, or decide whether it is random or not. Can such a concept, apart from mathematical and philosophical applications, have any <em class="EmphasisTypeItalic ">practical</em> applications? Perhaps surprisingly, the answer is “yes”. A large number of applications, ranging from philosophy to physics and biology, is discussed in the monograph by Li and Vitányi (<span class="CitationRef"><a epub:type="noteref" href="#CR16">2008</a></span>). Instead of attempting to give an overview of all applications, for which we do not have the space, we give an example of one striking application, namely the notion of information distance. Information distance is a notion built on Kolmogorov complexity that was introduced by Bennett et al. (<span class="CitationRef"><a epub:type="noteref" href="#CR4">1998</a></span>). It satisfies the properties of a metric (up to constants), and it gives a well-defined notion of distance between arbitrary pairs of binary strings. The computational status of information distance (and its normalized version) was unclear for a while, but as the notion of Kolmogorov complexity itself it turned out to be noncomputable (Terwijn et al. <span class="CitationRef"><a epub:type="noteref" href="#CR23">2011</a></span>). However, it is possible to <em class="EmphasisTypeItalic ">approximate</em> the ideal notion using existing, computable, compressors. This gives a computable approximation of information distance, that can in principle be applied to any pair of binary strings, be it musical files, the genetic code of mammals, or texts in any language. By computing the information distance between various files from a given domain, one can use the notion to classify anything that can be coded as a binary string. The results obtained in this way are startling. E.g. the method is able to correctly classify pieces of music by their composers, animals by their genetic code, or languages by their common roots, purely on the basis of similarity of their binary encodings, and without any expert knowledge. Apart from these applications, the notion of information distance is an example of a <em class="EmphasisTypeItalic ">provably</em> intractable notion, which nevertheless has important practical consequences. This provides a strong case for the study of such theoretical notions.</p></section><div class="License LicenseSubType-cc-by-nc"><a href="https://creativecommons.org/licenses/by-nc/2.5"><img alt="Creative Commons" src="../css/cc-by-nc.png"/></a><p class="SimplePara">&lt;SimplePara>&lt;Emphasis Type="Bold">Open Access&lt;/Emphasis> This chapter is licensed under the terms of the Creative Commons Attribution-NonCommercial 2.5 International License (http://creativecommons.org/licenses/by-nc/2.5/), which permits any noncommercial use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made. &lt;/SimplePara>
&lt;SimplePara>The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.&lt;/SimplePara></p></div><aside class="Appendix" epub:type="appendix" id="App1"><section class="Section1 RenderAsSection1" id="Sec12"><h2 class="Heading">Appendix A. Measure and Probability</h2><div class="Para" id="Par94">A <em class="EmphasisTypeItalic ">measure space</em> is a set <em class="EmphasisTypeItalic ">X</em> together with a function <span class="InlineEquation" id="IEq77"><img alt="$$\mu $$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq77.png" style="height:1.86em"/></span> that assigns positive real values <span class="InlineEquation" id="IEq78"><img alt="$$\mu (A)$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq78.png" style="height:1.86em"/></span> to subsets <em class="EmphasisTypeItalic ">A</em> of <em class="EmphasisTypeItalic ">X</em>, such that the following axioms are satisfied:<div class="OrderedList"><ol><li class="ListItem"><span class="ItemNumber">(i)</span><div class="ItemContent"><p class="Para" id="Par95">The empty set <span class="InlineEquation" id="IEq79"><img alt="$$\emptyset $$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq79.png" style="height:1.88em"/></span> has measure 0.</p></div><div class="ClearBoth"> </div></li><li class="ListItem"><span class="ItemNumber">(ii)</span><div class="ItemContent"><p class="Para" id="Par96">If <span class="InlineEquation" id="IEq80"><img alt="$$A\cap B = \emptyset $$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq80.png" style="height:1.88em"/></span>, then <span class="InlineEquation" id="IEq81"><img alt="$$\mu (A\cup B) = \mu (A) + \mu (B)$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq81.png" style="height:1.86em"/></span>. That is, if <em class="EmphasisTypeItalic ">A</em> and <em class="EmphasisTypeItalic ">B</em> are disjoint sets then the measure of their union is the sum of their measures.<sup><a epub:type="noteref" href="#Fn26" id="Fn26_source">26</a></sup>
</p></div><div class="ClearBoth"> </div></li></ol></div>
</div><p class="Para" id="Par98">If also <span class="InlineEquation" id="IEq82"><img alt="$$\mu (X)=1$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq82.png" style="height:1.86em"/></span> we can think of the values of <span class="InlineEquation" id="IEq83"><img alt="$$\mu $$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq83.png" style="height:1.86em"/></span> as <em class="EmphasisTypeItalic ">probabilities</em>, and we call <em class="EmphasisTypeItalic ">X</em> a <em class="EmphasisTypeItalic ">probability space</em>, and <span class="InlineEquation" id="IEq84"><img alt="$$\mu $$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq84.png" style="height:1.86em"/></span> a <em class="EmphasisTypeItalic ">probability measure</em>. If <em class="EmphasisTypeItalic ">A</em> is a subset of <em class="EmphasisTypeItalic ">X</em>, we think of <span class="InlineEquation" id="IEq85"><img alt="$$\mu (A)$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq85.png" style="height:1.86em"/></span> as the probability that a randomly chosen element of <em class="EmphasisTypeItalic ">X</em> will be in the set <em class="EmphasisTypeItalic ">A</em>. Subsets of <em class="EmphasisTypeItalic ">X</em> are also called <em class="EmphasisTypeItalic ">events</em>. In this setting the axioms (i) and (ii) are called the <em class="EmphasisTypeItalic ">Kolmogorov axioms</em> of probability. The axioms entail for example that if <span class="InlineEquation" id="IEq86"><img alt="$$A\subseteq B$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq86.png" style="height:1.86em"/></span>, i.e. the event <em class="EmphasisTypeItalic ">A</em> is contained in <em class="EmphasisTypeItalic ">B</em>, that then <span class="InlineEquation" id="IEq87"><img alt="$$\mu (A)\leqslant \mu (B)$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq87.png" style="height:1.86em"/></span>.</p><p class="Para" id="Par99">An important example of a probability space consists of the unit interval [0, 1] of the real line. The <em class="EmphasisTypeItalic ">uniform</em> or <em class="EmphasisTypeItalic ">Lebesgue</em> measure on [0, 1] is defined by assigning to every interval [<em class="EmphasisTypeItalic ">a</em>, <em class="EmphasisTypeItalic ">b</em>] the measure <span class="InlineEquation" id="IEq88"><img alt="$$b-a$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq88.png" style="height:1.86em"/></span>, i.e. the <em class="EmphasisTypeItalic ">length</em> of the interval. The measure of more complicated sets can be defined by considering combinations of intervals.<sup><a epub:type="noteref" href="#Fn27" id="Fn27_source">27</a></sup>
</p></section></aside><aside class="Appendix" epub:type="appendix" id="App2"><section class="Section1 RenderAsSection1" id="Sec13"><h2 class="Heading">Appendix B. The Noncomputability of the Complexity Function</h2><p class="Para" id="Par101">In Zvonkin and Levin (<span class="CitationRef"><a epub:type="noteref" href="#CR31">1970</a></span>) the following results are attributed to Kolmogorov.</p><div class="FormalPara FormalParaRenderingStyle1" id="FPar6"><h3 class="Heading">
                  <strong class="EmphasisTypeBold ">Theorem B.1</strong>
                </h3><p class="Para" id="Par102">The set <em class="EmphasisTypeItalic ">R</em> of Kolmogorov random strings does not contain any infinite c.e. set.<sup><a epub:type="noteref" href="#Fn28" id="Fn28_source">28</a></sup> In particular, <em class="EmphasisTypeItalic ">R</em> itself is not c.e.</p></div><div class="FormalPara FormalParaRenderingStyle2" id="FPar7"><h3 class="Heading">
                  <em class="EmphasisTypeItalic ">Proof</em>
                </h3><p class="Para" id="Par104">Suppose that <em class="EmphasisTypeItalic ">A</em> is an infinite c.e. subset of <em class="EmphasisTypeItalic ">R</em>. Consider the following procedure. Given a number <em class="EmphasisTypeItalic ">n</em>, find the first string <em class="EmphasisTypeItalic ">a</em> enumerated in <em class="EmphasisTypeItalic ">A</em> of length greater than <em class="EmphasisTypeItalic ">n</em>. Note that such a string <em class="EmphasisTypeItalic ">a</em> exists since <em class="EmphasisTypeItalic ">A</em> is infinite. Since <em class="EmphasisTypeItalic ">a</em> is effectively obtained from <em class="EmphasisTypeItalic ">n</em>, <em class="EmphasisTypeItalic ">n</em> serves as a description of <em class="EmphasisTypeItalic ">a</em>, and hence the Kolmogorov complexity <em class="EmphasisTypeItalic ">C</em>(<em class="EmphasisTypeItalic ">a</em>) is bounded by the length of <em class="EmphasisTypeItalic ">n</em>, which in binary notation is roughly <span class="InlineEquation" id="IEq89"><img alt="$$\log n$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq89.png" style="height:1.88em"/></span> (plus a fixed constant <em class="EmphasisTypeItalic ">c</em> independent of <em class="EmphasisTypeItalic ">n</em>, needed to describe the above procedure), where <span class="InlineEquation" id="IEq90"><img alt="$$\log $$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq90.png" style="height:1.88em"/></span> denotes the binary logarithm. So we have that <em class="EmphasisTypeItalic ">C</em>(<em class="EmphasisTypeItalic ">a</em>) is at most <span class="InlineEquation" id="IEq91"><img alt="$$\log n$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq91.png" style="height:1.88em"/></span>. But since <em class="EmphasisTypeItalic ">a</em> is random (because it is an element of <em class="EmphasisTypeItalic ">A</em>, which is a subset of <em class="EmphasisTypeItalic ">R</em>), we also have that <em class="EmphasisTypeItalic ">C</em>(<em class="EmphasisTypeItalic ">a</em>) is at least the length of <em class="EmphasisTypeItalic ">a</em>, which we chose to be greater than <em class="EmphasisTypeItalic ">n</em>. In summary, we have <span class="InlineEquation" id="IEq92"><img alt="$$n\leqslant C(a) \leqslant \log n + c$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq92.png" style="height:1.88em"/></span>, which is a contradiction for sufficiently large <em class="EmphasisTypeItalic ">n</em>.<span class="InlineEquation" id="IEq93"><img alt="$$\square $$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq93.png" style="height:0.96em"/></span></p></div><div class="FormalPara FormalParaRenderingStyle1" id="FPar8"><h3 class="Heading">
                  <strong class="EmphasisTypeBold ">Corollary B.2</strong>
                </h3><p class="Para" id="Par105">The complexity function <em class="EmphasisTypeItalic ">C</em> is not computable.</p></div><div class="FormalPara FormalParaRenderingStyle2" id="FPar9"><h3 class="Heading">
                  <em class="EmphasisTypeItalic ">Proof</em>
                </h3><p class="Para" id="Par106">If <em class="EmphasisTypeItalic ">C</em> were computable, we could generate an infinite set of random strings, contradicting Theorem <span class="InternalRef"><a href="#FPar6">B.1</a></span>.<span class="InlineEquation" id="IEq94"><img alt="$$\square $$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq94.png" style="height:0.96em"/></span></p></div><p class="Para" id="Par107"><strong class="EmphasisTypeBold ">Open Access</strong> This chapter is distributed under the terms of the Creative Commons Attribution-Noncommercial 2.5 License (<span class="ExternalRef"><a href="http://creativecommons.org/licenses/by-nc/2.5/"><span class="RefSource">http://​creativecommons.​org/​licenses/​by-nc/​2.​5/​</span></a></span>) which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source are credited. The images or other third party material in this chapter are included in the work’s Creative Commons license, unless indicated otherwise in the credit line; if such material is not included in the work’s Creative Commons license and the respective action is not permitted by statutory regulation, users will need to obtain permission from the license holder to duplicate, adapt or reproduce the material.</p></section></aside><aside class="Bibliography" epub:type="bibliography" id="Bib1"><h3 class="Heading">References</h3><ol class="BibliographyWrapper"><li class="Citation" epub:type="biblioentry"><div class="CitationContent" id="CR1">Agrawal, M., Kayal, N., &amp; Saxena, N. (2004). PRIMES is in P. <em class="EmphasisTypeItalic ">Annals of Mathematics</em>, <em class="EmphasisTypeItalic ">160</em>(2), 781–793.<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.4007/annals.2004.160.781"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry"><div class="CitationContent" id="CR2">Arora, S., &amp; Barak, B. (2009). <em class="EmphasisTypeItalic ">Computational complexity: A modern approach</em>. Cambridge University Press.</div></li><li class="Citation" epub:type="biblioentry"><div class="CitationContent" id="CR3">Becher, V., &amp; Slaman, T. A. (2014). On the normality of numbers in different bases. <em class="EmphasisTypeItalic ">Journal of the London Mathematical Society</em>, <em class="EmphasisTypeItalic ">90</em>(2), 472–494.<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1112/jlms/jdu035"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry"><div class="CitationContent" id="CR4">Bennett, C. H., Gács, P., Li, M., Vitányi, P. M. B., &amp; Zurek, W. (1998). Information distance. <em class="EmphasisTypeItalic ">IEEE Transactions on Information Theory</em>, <em class="EmphasisTypeItalic ">44</em>(4), 1407–1423.<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1109/18.681318"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry"><div class="CitationContent" id="CR5">Chaitin, G. J. (1974). Information-theoretic limitations of formal systems. <em class="EmphasisTypeItalic ">Journal of the ACM</em>, <em class="EmphasisTypeItalic ">21</em>, 403–424.<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1145/321832.321839"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry"><div class="CitationContent" id="CR6">Church, A. (1940). On the concept of a random sequence. <em class="EmphasisTypeItalic ">Bulletin of the American Mathematical Society</em>, <em class="EmphasisTypeItalic ">46</em>, 130–135.<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1090/S0002-9904-1940-07154-X"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry"><div class="CitationContent" id="CR7">Copeland, B. J. (Fall 2008 ed.). <em class="EmphasisTypeItalic ">The modern history of computing</em>. The Stanford Encyclopedia of Philosophy.</div></li><li class="Citation" epub:type="biblioentry"><div class="CitationContent" id="CR8">Downey, R. G., &amp; Hirschfeldt, D. R. (2010). <em class="EmphasisTypeItalic ">Algorithmic randomness and complexity</em>. Springer.<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1007/978-0-387-68441-3_11"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry"><div class="CitationContent" id="CR9">Downey, R. G., Hirschfeldt, D. R., Nies, A., &amp; Terwijn, S. A. (2006). Calibrating randomness. <em class="EmphasisTypeItalic ">Bulletin of Symbolic Logic</em>, <em class="EmphasisTypeItalic ">12</em>(3), 411–491.<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.2178/bsl/1154698741"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry"><div class="CitationContent" id="CR10">Doyle, P. G. (2011). <em class="EmphasisTypeItalic ">Maybe there’s no such thing as a random sequence</em>, manuscript.</div></li><li class="Citation" epub:type="biblioentry"><div class="CitationContent" id="CR11">Freudenthal, H. (1969). Realistic models in probability. In I. Lakatos (Ed.), <em class="EmphasisTypeItalic ">Problems in inductive logic</em>. North-Holland.</div></li><li class="Citation" epub:type="biblioentry"><div class="CitationContent" id="CR12">Gödel, K. (1940). <em class="EmphasisTypeItalic ">The consistency of the continuum-hypothesis</em>. Princeton University Press.</div></li><li class="Citation" epub:type="biblioentry"><div class="CitationContent" id="CR13">Jech, T. (2003). <em class="EmphasisTypeItalic ">Set theory</em> (3rd millennium ed.). Springer.</div></li><li class="Citation" epub:type="biblioentry"><div class="CitationContent" id="CR14">Kolmogorov, A. N. (1933). <em class="EmphasisTypeItalic ">Grundbegriffe der Wahrscheinlichkeitsrechnung</em>. Springer.</div></li><li class="Citation" epub:type="biblioentry"><div class="CitationContent" id="CR15">Kunen, K. (1980). <em class="EmphasisTypeItalic ">Set theory: An introduction to independence proofs</em>. North-Holland.</div></li><li class="Citation" epub:type="biblioentry"><div class="CitationContent" id="CR16">Li, M., &amp; Vitányi, P. (2008). <em class="EmphasisTypeItalic ">An introduction to Kolmogorov complexity and its applications</em> (3rd ed.). Springer.</div></li><li class="Citation" epub:type="biblioentry"><div class="CitationContent" id="CR17">Lutz, J. H. (1997). The quantitative structure of exponential time. In L. A. Hemaspaandra &amp; A. L. Selman (Eds.), <em class="EmphasisTypeItalic ">Complexity theory retrospective II</em> (pp. 225–254). Springer.<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1007/978-1-4612-1872-2_10"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry"><div class="CitationContent" id="CR18">Martin-Löf, P. (1966). The definition of random sequences. <em class="EmphasisTypeItalic ">Information and Control</em>, <em class="EmphasisTypeItalic ">9</em>, 602–619.<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1016/S0019-9958(66)80018-9"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry"><div class="CitationContent" id="CR19">Nies, A. (2009). <em class="EmphasisTypeItalic ">Computability and randomness</em>. Oxford University Press.<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1093/acprof:oso/9780199230761.003.0009"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry"><div class="CitationContent" id="CR20">Odifreddi, P. G. (1989). Classical recursion theory (Vol. 1). In <em class="EmphasisTypeItalic ">Studies in logic and the foundations of mathematics</em> (Vol. 125). North-Holland.</div></li><li class="Citation" epub:type="biblioentry"><div class="CitationContent" id="CR21">Schnorr, C. P. (1971). Zufälligkeit und Wahrscheinlichkeit. In <em class="EmphasisTypeItalic ">Lecture Notes in Mathematics</em> (Vol. 218). Springer.</div></li><li class="Citation" epub:type="biblioentry"><div class="CitationContent" id="CR22">Schnorr, C. P. (1973). Process complexity and effective random tests. <em class="EmphasisTypeItalic ">Journal of Computer and System Sciences</em>, <em class="EmphasisTypeItalic ">7</em>, 376–388.<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1016/S0022-0000(73)80030-3"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry"><div class="CitationContent" id="CR23">Terwijn, S. A., Torenvliet, L., &amp; Vitányi, P. M. B. (2011). Nonapproximability of the normalized information distance. <em class="EmphasisTypeItalic ">Journal of Computer and System Sciences</em>, <em class="EmphasisTypeItalic ">77</em>, 738–742.<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1016/j.jcss.2010.06.018"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry"><div class="CitationContent" id="CR24">Turing, A. M. (1936). On computable numbers with an application to the Entscheidungsproblem. In <em class="EmphasisTypeItalic ">Proceedings of the London Mathematical Society</em> (Vol. 42, pp. 230–265). Correction in <em class="EmphasisTypeItalic ">Proceedings of the London Mathematical Society</em> (Vol. 43, pp. 544–546) (1937).</div></li><li class="Citation" epub:type="biblioentry"><div class="CitationContent" id="CR25">van Lambalgen, M. (1987). <em class="EmphasisTypeItalic ">Random Sequences</em>. Ph.D. thesis, University of Amsterdam.</div></li><li class="Citation" epub:type="biblioentry"><div class="CitationContent" id="CR26">van Lambalgen, M. (1989). Algorithmic information theory. <em class="EmphasisTypeItalic ">Journal of Symbolic Logic</em>, <em class="EmphasisTypeItalic ">54</em>(4), 1389–1400.<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1017/S0022481200041153"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry"><div class="CitationContent" id="CR27">Ville, J. (1939). <em class="EmphasisTypeItalic ">Étude critique de la notion de collectif</em>, Monographies des Probabilités, Calcul des Probabilités et ses Applications, Gauthier-Villars.</div></li><li class="Citation" epub:type="biblioentry"><div class="CitationContent" id="CR28">von Mises, R. (1919). Grundlagen der Wahrscheinlichkeitsrechnung. <em class="EmphasisTypeItalic ">Mathematische Zeitschrift</em>, <em class="EmphasisTypeItalic ">5</em>, 52–99.<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1007/BF01203155"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry"><div class="CitationContent" id="CR29">Wald, A. (1936). Sur la notion de collectif dans la calcul des probabilités. <em class="EmphasisTypeItalic ">Comptes Rendus des Seances de l’Académie des Sciences</em>, <em class="EmphasisTypeItalic ">202</em>, 180–183.</div></li><li class="Citation" epub:type="biblioentry"><div class="CitationContent" id="CR30">Wald, A. (1937). Die Wiederspruchsfreiheit des Kollektivbegriffes der Wahrscheinlichkeitsrechnung. <em class="EmphasisTypeItalic ">Ergebnisse eines Mathematischen Kolloquiums</em>, <em class="EmphasisTypeItalic ">8</em>, 38–72.</div></li><li class="Citation" epub:type="biblioentry"><div class="CitationContent" id="CR31">Zvonkin, A. K., &amp; Levin, L. A. (1970). The complexity of finite objects and the development of the concepts of information and randomness by means of the theory of algorithms. <em class="EmphasisTypeItalic ">Russian Mathematical Surveys</em>, <em class="EmphasisTypeItalic ">25</em>(6), 83–124.<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1070/RM1970v025n06ABEH001269"><span><span>Crossref</span></span></a></span></span></div></li></ol></aside><aside class="FootnoteSection" epub:type="footnotes"><div class="Heading">Footnotes</div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn1_source">1</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn1"><p class="Para" id="Par5">As with many anecdotes of this kind, it is highly questionable if these words were really spoken, but the message they convey is nevertheless true.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn2_source">2</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn2"><p class="Para" id="Par8">The author actually took the trouble of doing this. We could have tried to write down a random sequence ourselves, but it is known that humans are notoriously bad at producing random sequences, and such sequences can usually be recognized by the fact that most people avoid long subsequences of zero’s, feeling that after three or four zero’s it is really time for a one. Indeed, depending on one’s temperament, some people may feel that the first four zero’s in the above sequence look suspicious.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn3_source">3</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn3"><p class="Para" id="Par13">The notion of mathematical definability is <em class="EmphasisTypeItalic ">itself</em> definable in set theory, see Kunen (<span class="CitationRef"><a epub:type="noteref" href="#CR15">1980</a></span>, Chap. V). If “random” is equated with “not definable”, then the following problem arises: By a result of Gödel (<span class="CitationRef"><a epub:type="noteref" href="#CR12">1940</a></span>) <em class="EmphasisTypeItalic ">it is consistent with the axioms of set theory that all sets are definable</em>, and hence the notion of randomness becomes empty. The solution to this problem is to be more modest in defining randomness, by only considering more restricted classes of sets, as is explained in what follows.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn4_source">4</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn4"><p class="Para" id="Par15">For a more elaborate discussion of the notion of Kollektiv see van Lambalgen (<span class="CitationRef"><a epub:type="noteref" href="#CR25">1987</a></span>).</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn5_source">5</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn5"><p class="Para" id="Par17">A set is called <em class="EmphasisTypeItalic ">countable</em> if its elements can be indexed by the natural numbers <span class="InlineEquation" id="IEq2"><img alt="$$0, 1, 2, 3, \ldots $$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq2.png" style="height:1.86em"/></span> These sets represent the smallest kind of infinity in the hierarchy of infinite sets.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn6_source">6</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn6"><p class="Para" id="Par20">In the light of the defects in the definition of Mises–Wald–Church random sequences, these sequences are nowadays called <em class="EmphasisTypeItalic ">stochastic</em> rather than random.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn7_source">7</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn7"><p class="Para" id="Par24">It is interesting to note that the Turing machine model has been a blueprint for all modern electronic computers. In particular, instead of performing specific algorithms, Turing machines are <em class="EmphasisTypeItalic ">universally programmable</em>, i.e. any algorithmic procedure can be implemented on them. Thus, the theory of computation <em class="EmphasisTypeItalic ">preceded</em> the actual building of electronic computers, and the fact that the first computers were universally programmable was directly influenced by it (cf. Copeland <span class="CitationRef"><a epub:type="noteref" href="#CR7">2008</a></span>). This situation is currently being repeated in the area of <em class="EmphasisTypeItalic ">quantum computing</em>, were the theory is being developed before any actual quantum computers have been built (see e.g. Arora and Barak <span class="CitationRef"><a epub:type="noteref" href="#CR2">2009</a></span>).</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn8_source">8</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn8"><p class="Para" id="Par25">The statement that the informal and the formal notions of computability coincide is the content of the so-called <em class="EmphasisTypeItalic ">Church-Turing thesis</em>, cf. Odifreddi (<span class="CitationRef"><a epub:type="noteref" href="#CR20">1989</a></span>) for a discussion.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn9_source">9</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn9"><p class="Para" id="Par29">In <span class="CitationRef"><a epub:type="noteref" href="#CR24">1936</a></span>, Turing used the undecidability of the Halting Problem to show the undecidability of the <em class="EmphasisTypeItalic ">Entscheidungsproblem</em>, that says (in modern terminology) that first-order predicate logic is undecidable.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn10_source">10</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn10"><p class="Para" id="Par34">This is not difficult to see: Since both <em class="EmphasisTypeItalic ">U</em> and <span class="InlineEquation" id="IEq7"><img alt="$$U'$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq7.png" style="height:1.89em"/></span> are universal, they can simulate each other, and any description of <em class="EmphasisTypeItalic ">x</em> relative to <em class="EmphasisTypeItalic ">U</em> can be translated into a description relative to <span class="InlineEquation" id="IEq8"><img alt="$$U'$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq8.png" style="height:1.89em"/></span> using only a fixed constant number of extra steps, where this constant is independent of <em class="EmphasisTypeItalic ">x</em>.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn11_source">11</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn11"><p class="Para" id="Par38">Notice that the definition requires the description to be a string of 0’s and 1’s, but we can easily convert a description in natural language into such a string by using a suitable coding, that only changes the length of descriptions by a small constant factor. Indeed, the theory described in this chapter applies to anything that can be represented or coded by binary strings, which includes many familiar mathematical objects such as numbers, sets, and graphs, but also objects such as DNA strings or texts in any language.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn12_source">12</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn12"><p class="Para" id="Par50">The proof runs as follows: There are only countably many algorithms, hence there are only countably many events of effective measure 0, and in measure theory a countable collection of measure 0 sets is again of measure 0.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn13_source">13</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn13"><p class="Para" id="Par53">We ignore here that decimal expansions in general are not unique, for example <span class="InlineEquation" id="IEq20"><img alt="$$0,999\ldots = 1,000\ldots $$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq20.png" style="height:1.86em"/></span>, but this is immaterial.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn14_source">14</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn14"><p class="Para" id="Par62">The word “martingale” comes from gambling theory, where it refers to the very dangerous strategy of doubling the stakes in every round of gambling, until a win occurs. With the stakes growing exponentially, if the win does not occur quickly enough, this may result in an astronomical loss for the gambler. In modern probability theory, the word “martingale” refers to a betting strategy in general.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn15_source">15</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn15"><p class="Para" id="Par66">Ville showed that martingales provide an alternative, game-theoretic, formulation of measure theory: The sets of measure 0 are precisely the sets on which a martingale can win an infinite amount of money.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn16_source">16</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn16"><p class="Para" id="Par68">Note that for every sequence <em class="EmphasisTypeItalic ">X</em> there is a martingale that wins an infinite amount of capital on <em class="EmphasisTypeItalic ">X</em>: just set <span class="InlineEquation" id="IEq61"><img alt="$$d(X(0)\ldots X(n-1)\widehat{{}}i) = 2 d( X(0)\ldots X(n-1))$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq61.png" style="height:2em"/></span>, where <span class="InlineEquation" id="IEq62"><img alt="$$i = X(n)$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq62.png" style="height:1.86em"/></span>. However, in order to play this strategy, one has to have full knowledge of <em class="EmphasisTypeItalic ">X</em>.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn17_source">17</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn17"><p class="Para" id="Par69">C.e. is an abbreviation of “computably enumerable”. This notion is further explained in Sect. <span class="InternalRef"><a href="#Sec8">8</a></span>.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn18_source">18</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn18"><p class="Para" id="Par72">This follows by the method of arithmetization: Statements about Turing machines can be translated into arithmetic by coding. If the set of provable formulas were decidable, it would follow that the Halting Problem is also decidable.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn19_source">19</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn19"><p class="Para" id="Par73">We can do this by considering all possible pairs of Turing machines <em class="EmphasisTypeItalic ">M</em> and inputs <em class="EmphasisTypeItalic ">x</em>, and running all of them in parallel. Every time we see a computation <em class="EmphasisTypeItalic ">M</em>(<em class="EmphasisTypeItalic ">x</em>) converge, we add it to the list. Note, however, that we cannot list the converging computations in order, since there is no way to predict the running time of a converging computation. Indeed, if we could list the converging computations in order, the Halting Problem would be decidable.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn20_source">20</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn20"><p class="Para" id="Par78">As for Gödel’s incompleteness theorem, the statement holds for any reasonable formal system that is able to express elementary arithmetic. In fact, it follows from Theorem <span class="InternalRef"><a href="#FPar6">B.1</a></span> that any such system can prove the randomness of at most <em class="EmphasisTypeItalic ">finitely</em> many strings.</p><p class="Para" id="Par79">Chaitin also drew a number of dubious philosophical conclusions from his version of the incompleteness theorem, that were adequately refuted by van Lambalgen (<span class="CitationRef"><a epub:type="noteref" href="#CR26">1989</a></span>), and later in more detail by Ratikaainen, Franzen, Porter, and others. Unfortunately, this has not prevented Chaitin’s claims from being widely publicized.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn21_source">21</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn21"><p class="Para" id="Par82">After Martin-Löf’s paper (<span class="CitationRef"><a epub:type="noteref" href="#CR18">1966</a></span>), the notion of Martin-Löf randomness became known as a notion of “computable randomness”. As Schnorr observed, this was not quite correct, and for example the characterization with c.e.-martingales pointed out that is was more apt to think of it as “c.e.-randomness”. To obtain a notion of “computable randomness”, extra computational restrictions have to be imposed. Schnorr did this by basing his notion on Brouwer’s notion of constructive measure zero set. The resulting notion of randomness, nowadays called Schnorr randomness, has become one of the standard notions in randomness theory, see Downey and Hirschfeldt (<span class="CitationRef"><a epub:type="noteref" href="#CR8">2010</a></span>).</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn22_source">22</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn22"><p class="Para" id="Par88">The Monte Carlo method was first used extensively in the work of Ulam and von Neumann on the hydrogen bomb.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn23_source">23</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn23"><p class="Para" id="Par90">In computational complexity theory, an algorithms is considered feasible if it works in <em class="EmphasisTypeItalic ">polynomial time</em>, that is, if on an input of length <em class="EmphasisTypeItalic ">n</em> it takes <span class="InlineEquation" id="IEq76"><img alt="$$n^k$$" src="../images/340096_1_En_3_Chapter/340096_1_En_3_Chapter_TeX_IEq76.png" style="height:2.19em"/></span> computation steps for some fixed constant <em class="EmphasisTypeItalic ">k</em>.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn24_source">24</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn24"><p class="Para" id="Par91">Since 2001 there also exist <em class="EmphasisTypeItalic ">deterministic</em> feasible algorithms to determine primality (Agrawal et al. <span class="CitationRef"><a epub:type="noteref" href="#CR1">2004</a></span>), but the randomized algorithms are still faster, and since their probability of error can be made arbitrary small, in practice they are still the preferred method.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn25_source">25</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn25"><p class="Para" id="Par92">The question about derandomization is embodied in the relation between the complexity classes P and BPP, see Arora and Barak (<span class="CitationRef"><a epub:type="noteref" href="#CR2">2009</a></span>). This is a probabilistic version of the notorious P versus NP problem, which is about determinism versus nondeterminism. The latter is one of the most famous open problems in mathematics.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn26_source">26</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn26"><p class="Para" id="Par97">It is in fact usually required that this property also holds for countably infinite collections.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn27_source">27</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn27"><p class="Para" id="Par100">The definition of a probability measure on the unit interval [0, 1] that assigns a probability to <em class="EmphasisTypeItalic ">all</em> subsets of it is fraught with technical difficulties that we will not discuss here. This problem, the so-called <em class="EmphasisTypeItalic ">measure problem</em>, properly belongs to the field of set theory, and has led to deep insights into the nature of sets and their role in the foundation of mathematics (cf. Jech <span class="CitationRef"><a epub:type="noteref" href="#CR13">2003</a></span>).</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn28_source">28</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn28"><p class="Para" id="Par103"><em class="EmphasisTypeItalic ">R</em> itself is infinite, but by the theorem there is no way to effectively generate infinitely many elements from it. Such sets are called <em class="EmphasisTypeItalic ">immune</em>.</p></div><div class="ClearBoth"> </div></div></aside></div></body></html>